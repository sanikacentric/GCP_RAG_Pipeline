{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Infrastructure as Code (Terraform - infra/main.tf) - This Terraform script sets up the core GCP services. A real production setup would be split into modules."
      ],
      "metadata": {
        "id": "GBS257sGyL4q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mww9UOuxyKNq"
      },
      "outputs": [],
      "source": [
        "terraform {\n",
        "  required_providers {\n",
        "    google = {\n",
        "      source  = \"hashicorp/google\"\n",
        "      version = \"~> 4.0\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "provider \"google\" {\n",
        "  project = var.project_id\n",
        "  region  = var.region\n",
        "}\n",
        "\n",
        "# Cloud Storage Bucket for raw documents\n",
        "resource \"google_storage_bucket\" \"documents_bucket\" {\n",
        "  name          = \"${var.project_id}-documents\"\n",
        "  location      = var.region\n",
        "  force_destroy = false\n",
        "\n",
        "  versioning {\n",
        "    enabled = true\n",
        "  }\n",
        "\n",
        "  uniform_bucket_level_access = true\n",
        "\n",
        "  encryption {\n",
        "    default_kms_key_name = google_kms_crypto_key.documents_key.id\n",
        "  }\n",
        "}\n",
        "\n",
        "# Pub/Sub Topic for new file notifications\n",
        "resource \"google_pubsub_topic\" \"ingestion_topic\" {\n",
        "  name = \"document-ingestion-topic\"\n",
        "}\n",
        "\n",
        "# Trigger Pub/Sub message on new file upload\n",
        "resource \"google_storage_notification\" \"documents_notification\" {\n",
        "  bucket         = google_storage_bucket.documents_bucket.name\n",
        "  payload_format = \"JSON_API_V1\"\n",
        "  topic          = google_pubsub_topic.ingestion_topic.id\n",
        "  event_types    = [\"OBJECT_FINALIZE\"]\n",
        "  depends_on     = [google_pubsub_topic_iam_binding.binding]\n",
        "}\n",
        "\n",
        "# BigQuery Dataset for processed text and metadata\n",
        "resource \"google_bigquery_dataset\" \"rag_metadata\" {\n",
        "  dataset_id    = \"rag_metadata\"\n",
        "  friendly_name = \"RAG Processing Metadata\"\n",
        "  description   = \"Stores cleaned text chunks and processing logs for the RAG pipeline.\"\n",
        "  location      = var.region\n",
        "\n",
        "  encryption_key {\n",
        "    kms_key_name = google_kms_crypto_key.bq_key.id\n",
        "  }\n",
        "}\n",
        "\n",
        "# Vertex AI Feature Store for Embeddings (Fully managed index is created via SDK, but we define the VPC here)\n",
        "resource \"google_vertex_ai_index\" \"policy_embeddings_index\" {\n",
        "  region       = var.region\n",
        "  display_name = \"policy-documents-index\"\n",
        "  description  = \"Index for semantic search over policy document chunks\"\n",
        "  index_update_method = \"BATCH_UPDATE\"\n",
        "\n",
        "  metadata {\n",
        "    contents_delta_uri = \"gs://${google_storage_bucket.documents_bucket.name}/embeddings/\"\n",
        "    config {\n",
        "      dimensions = 768\n",
        "      algorithm_config {\n",
        "        tree_ah_config {\n",
        "          leaf_node_embedding_count = 1000\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "# KMS Keys for Encryption\n",
        "resource \"google_kms_key_ring\" \"key_ring\" {\n",
        "  name     = \"rag-pipeline-keyring\"\n",
        "  location = \"global\"\n",
        "}\n",
        "\n",
        "resource \"google_kms_crypto_key\" \"documents_key\" {\n",
        "  name            = \"documents-bucket-key\"\n",
        "  key_ring        = google_kms_key_ring.key_ring.id\n",
        "  rotation_period = \"7776000s\" # 90 days\n",
        "\n",
        "  version_template {\n",
        "    algorithm = \"GOOGLE_SYMMETRIC_ENCRYPTION\"\n",
        "  }\n",
        "}\n",
        "\n",
        "resource \"google_kms_crypto_key\" \"bq_key\" {\n",
        "  name            = \"bigquery-key\"\n",
        "  key_ring        = google_kms_key_ring.key_ring.id\n",
        "  rotation_period = \"7776000s\"\n",
        "  version_template {\n",
        "    algorithm = \"GOOGLE_SYMMETRIC_ENCRYPTION\"\n",
        "  }\n",
        "}\n",
        "\n",
        "# ... (Additional IAM bindings, VPC-SC perimeters, etc. would be defined here)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ingestion & Processing Pipeline (Apache Beam - ingestion/beam_pipeline.py)\n",
        "This pipeline is triggered by a Pub/Sub message from the storage bucket."
      ],
      "metadata": {
        "id": "tLC3DeiVyWfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from google.api_core.exceptions import GoogleAPIError\n",
        "from google.cloud import documentai, storage, bigquery\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Define the BigQuery table schema for the cleaned chunks\n",
        "BQ_SCHEMA = {\n",
        "    \"fields\": [\n",
        "        {\"name\": \"doc_id\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n",
        "        {\"name\": \"chunk_id\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n",
        "        {\"name\": \"content\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
        "        {\"name\": \"page_number\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
        "        {\"name\": \"section_type\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
        "        {\"name\": \"policy_type\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
        "        {\"name\": \"jurisdiction\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
        "        {\"name\": \"effective_date\", \"type\": \"DATE\", \"mode\": \"NULLABLE\"},\n",
        "        {\"name\": \"processing_timestamp\", \"type\": \"TIMESTAMP\", \"mode\": \"REQUIRED\"},\n",
        "    ]\n",
        "}\n",
        "\n",
        "class ProcessDocument(beam.DoFn):\n",
        "    \"\"\"A DoFn to process a document from GCS using Document AI.\"\"\"\n",
        "    def __init__(self, processor_name):\n",
        "        self.processor_name = processor_name\n",
        "\n",
        "    def setup(self):\n",
        "        self.docai_client = documentai.DocumentProcessorServiceClient()\n",
        "        self.storage_client = storage.Client()\n",
        "\n",
        "    def process(self, message):\n",
        "        data = json.loads(message)\n",
        "        bucket_name = data['bucket']\n",
        "        file_path = data['name']\n",
        "\n",
        "        try:\n",
        "            # 1. Read file from GCS\n",
        "            bucket = self.storage_client.bucket(bucket_name)\n",
        "            blob = bucket.blob(file_path)\n",
        "            file_content = blob.download_as_bytes()\n",
        "\n",
        "            # 2. Process with Document AI\n",
        "            request = documentai.ProcessRequest(\n",
        "                name=self.processor_name,\n",
        "                raw_document=documentai.RawDocument(\n",
        "                    content=file_content, mime_type=blob.content_type\n",
        "                ),\n",
        "            )\n",
        "            result = self.docai_client.process_document(request=request)\n",
        "            full_text = result.document.text\n",
        "\n",
        "            # 3. Custom Preprocessing (simplified example)\n",
        "            metadata = self._extract_metadata(result.document, file_path)\n",
        "            chunks = self._chunk_text(full_text)\n",
        "\n",
        "            # 4. Yield chunks for BigQuery and Embedding generation\n",
        "            for i, chunk_text in enumerate(chunks):\n",
        "                chunk_id = f\"{file_path}#chunk{i}\"\n",
        "                row = {\n",
        "                    \"doc_id\": file_path,\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"content\": chunk_text,\n",
        "                    \"page_number\": 1,  # Simplified, would be parsed from entities\n",
        "                    \"section_type\": metadata.get('section', 'body'),\n",
        "                    \"policy_type\": metadata.get('policy_type', 'unknown'),\n",
        "                    \"jurisdiction\": metadata.get('jurisdiction', 'global'),\n",
        "                    \"effective_date\": metadata.get('effective_date'),\n",
        "                    \"processing_timestamp\": beam.window.TimestampedValue(i, beam.DoFn.TimestampParam),\n",
        "                }\n",
        "                yield row\n",
        "\n",
        "        except GoogleAPIError as e:\n",
        "            # Log error to Dead Letter Queue (DLQ) or Cloud Logging\n",
        "            error_row = {\"error\": str(e), \"file_path\": file_path, \"message\": message}\n",
        "            yield beam.pvalue.TaggedOutput('errors', error_row)\n",
        "\n",
        "    def _extract_metadata(self, document, file_path):\n",
        "        \"\"\"Extract metadata from Document AI entities and filename.\"\"\"\n",
        "        metadata = {}\n",
        "        for entity in document.entities:\n",
        "            if entity.type_ == 'policy_type':\n",
        "                metadata['policy_type'] = entity.mention_text\n",
        "            elif entity.type_ == 'effective_date':\n",
        "                metadata['effective_date'] = entity.mention_text # Would normalize to date\n",
        "            # ... extract other entities\n",
        "        return metadata\n",
        "\n",
        "    def _chunk_text(self, text, chunk_size=512, overlap=50):\n",
        "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "        # Use a more sophisticated chunking strategy (e.g., semantic, markdown-aware)\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        for i in range(0, len(words), chunk_size - overlap):\n",
        "            chunk = ' '.join(words[i:i + chunk_size])\n",
        "            chunks.append(chunk)\n",
        "        return chunks\n",
        "\n",
        "def run():\n",
        "    options = PipelineOptions(\n",
        "        streaming=True,\n",
        "        save_main_session=True,\n",
        "        region='us-central1',\n",
        "        project=project_id,\n",
        "        temp_location=f'gs://{bucket_name}/temp'\n",
        "    )\n",
        "\n",
        "    processor_name = \"projects/my-project/locations/us-central1/processors/abc123\" # Your Document AI Processor\n",
        "\n",
        "    with beam.Pipeline(options=options) as p:\n",
        "        # Read from Pub/Sub\n",
        "        messages = (p | \"Read from Pub/Sub\" >> beam.io.ReadFromPubSub(\n",
        "            topic=f'projects/{project_id}/topics/document-ingestion-topic')\n",
        "        )\n",
        "\n",
        "        processed_data = (messages\n",
        "                          | \"Process Document\" >> beam.ParDo(\n",
        "                                ProcessDocument(processor_name=processor_name)\n",
        "                            ).with_outputs('errors', main='main')\n",
        "                          )\n",
        "\n",
        "        # Write clean chunks to BigQuery\n",
        "        _ = (processed_data.main\n",
        "             | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
        "                 table=f'{project_id}:rag_metadata.document_chunks',\n",
        "                 schema=BQ_SCHEMA,\n",
        "                 write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
        "                 create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n",
        "             )\n",
        "\n",
        "        # Write errors to a DLQ in Cloud Storage or BigQuery\n",
        "        _ = (processed_data.errors\n",
        "             | \"Write Errors to DLQ\" >> beam.io.WriteToText(\n",
        "                 file_path_prefix=f'gs://{bucket_name}/dlq/errors',\n",
        "                 file_name_suffix='.json')\n",
        "             )\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "metadata": {
        "id": "WlPEoTyeyXYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding/embedding.py\n",
        "This script is responsible for:\n",
        "\n",
        "Reading the cleaned text chunks from BigQuery.\n",
        "\n",
        "Generating embeddings for each chunk using the Vertex AI Text Embedding API.\n",
        "\n",
        "Upserting the embeddings and their metadata into the Vertex AI Vector Search index."
      ],
      "metadata": {
        "id": "oMwegbeEykiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "embedding.py\n",
        "\n",
        "A production-level script to generate embeddings for cleaned document chunks\n",
        "stored in BigQuery and upsert them into a Vertex AI Vector Search index.\n",
        "\n",
        "This script is designed to be run as a batch job (e.g., via Cloud Scheduler,\n",
        "triggered by Pub/Sub after a Dataflow job completes, or as a Cloud Function).\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "from google.cloud import bigquery, aiplatform, storage\n",
        "from google.cloud.aiplatform import vector_search\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "import backoff\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration (would be set via environment variables or a config file in production)\n",
        "PROJECT_ID = \"your-project-id\"\n",
        "REGION = \"us-central1\"\n",
        "BQ_DATASET = \"rag_metadata\"\n",
        "BQ_TABLE = \"document_chunks\"\n",
        "BATCH_SIZE = 100  # Adjust based on API quotas and performance\n",
        "INDEX_ENDPOINT_ID = \"your-index-endpoint-id\"  # e.g., \"123456789\"\n",
        "DEPLOYED_INDEX_ID = \"policy_documents_index\"  # The ID you gave your index on deployment\n",
        "\n",
        "class EmbeddingGenerator:\n",
        "    \"\"\"Orchestrates the generation and upsert of embeddings to Vector Search.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize clients for BigQuery, Vertex AI, and Cloud Storage.\"\"\"\n",
        "        self.bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "        self.embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
        "        # Initialize the AI Platform SDK for the Matching Engine\n",
        "        aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "        # Initialize the Vector Search client\n",
        "        self.vector_search_client = vector_search.VectorSearchServiceClient()\n",
        "        self.index_endpoint = self._get_index_endpoint(INDEX_ENDPOINT_ID)\n",
        "\n",
        "    def _get_index_endpoint(self, index_endpoint_id: str):\n",
        "        \"\"\"Fetches the index endpoint resource.\"\"\"\n",
        "        endpoint_name = self.vector_search_client.index_endpoint_path(\n",
        "            project=PROJECT_ID, location=REGION, index_endpoint=index_endpoint_id\n",
        "        )\n",
        "        return self.vector_search_client.get_index_endpoint(name=endpoint_name)\n",
        "\n",
        "    @backoff.on_exception(backoff.expo, Exception, max_tries=5)\n",
        "    def _generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"\n",
        "        Generates embeddings for a batch of text chunks using Vertex AI.\n",
        "        Uses exponential backoff for retrying on API errors.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            embeddings = self.embedding_model.get_embeddings(texts)\n",
        "            return [e.values for e in embeddings]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate embeddings for batch: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _prepare_upsert_data(\n",
        "        self,\n",
        "        df_batch: pd.DataFrame,\n",
        "        embeddings: List[List[float]]\n",
        "    ) -> List[vector_search.IndexDatapoint]:\n",
        "        \"\"\"\n",
        "        Prepares the data for upsert into the Vector Search index.\n",
        "        Maps each chunk's data and embedding to an IndexDatapoint object.\n",
        "        \"\"\"\n",
        "        datapoints = []\n",
        "        for _, row in df_batch.iterrows():\n",
        "            # The datapoint_id must be unique and stable for each chunk.\n",
        "            # Using the chunk_id from BigQuery is perfect.\n",
        "            datapoint_id = row[\"chunk_id\"]\n",
        "\n",
        "            # Create the embedding array\n",
        "            embedding = vector_search.types.IndexDatapoint(\n",
        "                datapoint_id=datapoint_id,\n",
        "                feature_vector=embeddings.pop(0), # Get the first embedding for this row\n",
        "                restricts=[\n",
        "                    # Add metadata for filtering during query.\n",
        "                    # This is a key performance feature.\n",
        "                    vector_search.types.IndexDatapoint.Restriction(\n",
        "                        namespace=\"policy_type\",\n",
        "                        allow_list=[row[\"policy_type\"]] if pd.notna(row[\"policy_type\"]) else [],\n",
        "                    ),\n",
        "                    vector_search.types.IndexDatapoint.Restriction(\n",
        "                        namespace=\"jurisdiction\",\n",
        "                        allow_list=[row[\"jurisdiction\"]] if pd.notna(row[\"jurisdiction\"]) else [],\n",
        "                    ),\n",
        "                    vector_search.types.IndexDatapoint.Restriction(\n",
        "                        namespace=\"section_type\",\n",
        "                        allow_list=[row[\"section_type\"]] if pd.notna(row[\"section_type\"]) else [],\n",
        "                    ),\n",
        "                ],\n",
        "                # Optional: Store sparse embedding for hybrid search\n",
        "                # sparse_embedding=vector_search.types.IndexDatapoint.SparseEmbedding(...)\n",
        "            )\n",
        "            datapoints.append(datapoint)\n",
        "        return datapoints\n",
        "\n",
        "    def _upsert_datapoints_batch(self, datapoints: List[vector_search.types.IndexDatapoint]):\n",
        "        \"\"\"Upserts a batch of datapoints to the Vector Search index.\"\"\"\n",
        "        try:\n",
        "            # The full resource name of the deployed index\n",
        "            deployed_index_name = self.index_endpoint.deployed_indexes[DEPLOYED_INDEX_ID].id\n",
        "\n",
        "            upsert_request = vector_search.types.UpsertDatapointsRequest(\n",
        "                index_endpoint=self.index_endpoint.name,\n",
        "                deployed_index_id=deployed_index_name,\n",
        "                datapoints=datapoints,\n",
        "            )\n",
        "\n",
        "            response = self.vector_search_client.upsert_datapoints(upsert_request)\n",
        "            # Log if any datapoints failed to upsert\n",
        "            if response.failed_datapoints:\n",
        "                for failed in response.failed_datapoints:\n",
        "                    logger.warning(f\"Failed to upsert datapoint {failed.datapoint_id}: {failed.error_message}\")\n",
        "            else:\n",
        "                logger.info(f\"Successfully upserted batch of {len(datapoints)} datapoints.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to upsert batch to index: {e}\")\n",
        "            # In production, you might push failed batches to a retry queue (e.g., Pub/Sub)\n",
        "\n",
        "    def process_new_chunks(self, last_processed_timestamp: str = None):\n",
        "        \"\"\"\n",
        "        Main method to process chunks from BigQuery.\n",
        "        If a timestamp is provided, only processes chunks newer than that time.\n",
        "        Otherwise, processes all chunks (use with caution!).\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting embedding generation process.\")\n",
        "\n",
        "        # Build the BigQuery query\n",
        "        query = f\"\"\"\n",
        "            SELECT\n",
        "                doc_id,\n",
        "                chunk_id,\n",
        "                content,\n",
        "                policy_type,\n",
        "                jurisdiction,\n",
        "                section_type\n",
        "            FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\n",
        "            WHERE content IS NOT NULL\n",
        "            AND LENGTH(content) > 10  -- Filter out very short/empty chunks\n",
        "        \"\"\"\n",
        "        if last_processed_timestamp:\n",
        "            query += f\" AND processing_timestamp > '{last_processed_timestamp}'\"\n",
        "\n",
        "        query += \" ORDER BY processing_timestamp ASC\" # Process in order\n",
        "\n",
        "        # Use the BigQuery storage API for efficient large data reads\n",
        "        df_iterator = self.bq_client.list_rows(\n",
        "            self.bq_client.get_table(f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\"),\n",
        "            selected_fields=[...] # Specify fields to read\n",
        "        ).to_dataframe_iterable()\n",
        "\n",
        "        # Alternatively, use a query job for more complex filtering\n",
        "        query_job = self.bq_client.query(query)\n",
        "        total_rows = query_job.result().total_rows\n",
        "        logger.info(f\"Found {total_rows} chunks to process.\")\n",
        "\n",
        "        processed_count = 0\n",
        "        for df_batch in df_iterator:\n",
        "            # df_batch is a chunk of the result set\n",
        "            if df_batch.empty:\n",
        "                continue\n",
        "\n",
        "            texts = df_batch[\"content\"].tolist()\n",
        "\n",
        "            # 1. Generate Embeddings\n",
        "            logger.info(f\"Generating embeddings for batch of {len(texts)} chunks...\")\n",
        "            embeddings = self._generate_embeddings_batch(texts)\n",
        "\n",
        "            # 2. Prepare Data for Upsert\n",
        "            logger.info(\"Preparing datapoints for upsert...\")\n",
        "            datapoints = self._prepare_upsert_data(df_batch, embeddings)\n",
        "\n",
        "            # 3. Upsert to Vector Index\n",
        "            logger.info(\"Upserting batch to Vector Search index...\")\n",
        "            self._upsert_datapoints_batch(datapoints)\n",
        "\n",
        "            processed_count += len(texts)\n",
        "            logger.info(f\"Processed {processed_count}/{total_rows} chunks.\")\n",
        "\n",
        "            # Be kind to the API quotas\n",
        "            time.sleep(1)\n",
        "\n",
        "        logger.info(f\"Embedding generation complete. Total chunks processed: {processed_count}\")\n",
        "\n",
        "        # In production, you would now update a state tracker (e.g., a BigQuery table or Firestore document)\n",
        "        # with the latest `processing_timestamp` to use as `last_processed_timestamp` in the next run.\n",
        "\n",
        "def main(event, context):\n",
        "    \"\"\"Entry point for Cloud Function or Cloud Run Job.\"\"\"\n",
        "    # The event could be a Pub/Sub message from the Dataflow job completion topic\n",
        "    # or a Cloud Scheduler trigger.\n",
        "\n",
        "    # Example: Get the timestamp of the last successful run from a config store\n",
        "    # last_run_time = get_last_run_time()\n",
        "\n",
        "    # For a full refresh, set last_processed_timestamp to None\n",
        "    generator = EmbeddingGenerator()\n",
        "    generator.process_new_chunks(last_processed_timestamp=None)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # For local testing or containerized execution\n",
        "    main(None, None)"
      ],
      "metadata": {
        "id": "BhXMTXoIyl5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ". RAG Query API (FastAPI on Cloud Run - api/main.py & api/query_service.py)\n",
        "api/main.py"
      ],
      "metadata": {
        "id": "oWq40ra02uHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Depends, HTTPException, Security\n",
        "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
        "from google.cloud import bigquery\n",
        "from google.auth import jwt\n",
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "from . import models\n",
        "from .query_service import QueryService\n",
        "\n",
        "app = FastAPI(title=\"Policy RAG API\", version=\"1.0.0\")\n",
        "security = HTTPBearer()\n",
        "\n",
        "# Initialize clients (would be configured via environment variables)\n",
        "vertexai.init(project=\"your-project-id\", location=\"us-central1\")\n",
        "llm_model = TextGenerationModel.from_pretrained(\"gemini-1.5-pro\")\n",
        "query_service = QueryService()\n",
        "\n",
        "def validate_token(credentials: HTTPAuthorizationCredentials = Security(security)):\n",
        "    \"\"\"Validate JWT token using Google's public keys.\"\"\"\n",
        "    try:\n",
        "        # In production, use a library like `google-auth` to verify the token\n",
        "        # and check audience, issuer, and required scopes.\n",
        "        decoded_token = jwt.decode(credentials.credentials, verify=False)\n",
        "        return decoded_token.get('email')\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=401, detail=\"Invalid authentication credentials\")\n",
        "\n",
        "@app.post(\"/query\", response_model=models.QueryResponse)\n",
        "async def query_policies(\n",
        "    request: models.QueryRequest,\n",
        "    user_email: str = Depends(validate_token)\n",
        "):\n",
        "    \"\"\"Main endpoint for natural language queries.\"\"\"\n",
        "    try:\n",
        "        result = await query_service.execute_rag_query(\n",
        "            query=request.query,\n",
        "            user_id=user_email,\n",
        "            filter_dict=request.filters\n",
        "        )\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\"}\n",
        "\n",
        "# ... Additional endpoints for feedback, analytics, etc."
      ],
      "metadata": {
        "id": "FZmLOpc52vGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "api/query_service.py"
      ],
      "metadata": {
        "id": "RxhBkIIO2yeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform, bigquery\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "import logging\n",
        "from .models import QueryResponse, RetrievedChunk\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class QueryService:\n",
        "    def __init__(self):\n",
        "        self.vector_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
        "            index_endpoint_name=\"projects/my-project/locations/us-central1/indexEndpoints/123\"\n",
        "        )\n",
        "        self.llm_model = TextGenerationModel.from_pretrained(\"gemini-1.5-pro\")\n",
        "        self.bq_client = bigquery.Client()\n",
        "\n",
        "    async def execute_rag_query(self, query: str, user_id: str, filter_dict: dict = None):\n",
        "        # 1. Generate Embedding for the Query\n",
        "        query_embedding = await self._generate_embedding(query)\n",
        "\n",
        "        # 2. Retrieve Relevant Chunks from Vector Store\n",
        "        retrieved_chunks = self._retrieve_chunks(query_embedding, filter_dict, top_k=5)\n",
        "        if not retrieved_chunks:\n",
        "            return QueryResponse(answer=\"No relevant policy documents found.\", chunks=[])\n",
        "\n",
        "        # 3. Construct Context for LLM\n",
        "        context = \"\\n\\n---\\n\\n\".join([chunk.content for chunk in retrieved_chunks])\n",
        "\n",
        "        # 4. Generate Answer with Gemini, using a carefully engineered prompt\n",
        "        prompt = self._build_prompt(context, query)\n",
        "        answer = self._generate_answer(prompt)\n",
        "\n",
        "        # 5. Log the query for analytics and improvement\n",
        "        self._log_query(user_id, query, retrieved_chunks, answer)\n",
        "\n",
        "        return QueryResponse(answer=answer, chunks=retrieved_chunks)\n",
        "\n",
        "    def _retrieve_chunks(self, query_embedding, filters, top_k=5):\n",
        "        \"\"\"Query Vertex AI Vector Search.\"\"\"\n",
        "        try:\n",
        "            # The filter is a SQL-like string for metadata\n",
        "            filter_string = None\n",
        "            if filters:\n",
        "                # e.g., \"policy_type = 'claims' AND jurisdiction = 'NY'\"\n",
        "                filter_string = \" AND \".join([f\"{k} = '{v}'\" for k, v in filters.items()])\n",
        "\n",
        "            response = self.vector_index_endpoint.find_neighbors(\n",
        "                deployed_index_id=\"policy_index\",\n",
        "                queries=[query_embedding],\n",
        "                num_neighbors=top_k,\n",
        "                filter=filter_string\n",
        "            )\n",
        "\n",
        "            retrieved_chunks = []\n",
        "            for neighbor in response[0]:\n",
        "                # Assuming metadata is stored in the index\n",
        "                chunk = RetrievedChunk(\n",
        "                    content=neighbor.datapoint.datapoint_id, # or stored text\n",
        "                    document_id=neighbor.datapoint.restricts[0].namespace, # example\n",
        "                    confidence_score=neighbor.distance,\n",
        "                    # ... other metadata\n",
        "                )\n",
        "                retrieved_chunks.append(chunk)\n",
        "            return retrieved_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error retrieving from vector index: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _build_prompt(self, context, query):\n",
        "        \"\"\"Build a structured prompt to guide the LLM and minimize hallucination.\"\"\"\n",
        "        return f\"\"\"\n",
        "        You are a helpful and precise assistant for a large insurance company.\n",
        "        Always answer based ONLY on the provided context from the company's policy documents.\n",
        "        If the answer is not contained in the context, say \"I cannot find a specific policy regarding this.\"\n",
        "\n",
        "        Context from Policy Documents:\n",
        "        {context}\n",
        "\n",
        "        User Question: {query}\n",
        "\n",
        "        Instructions:\n",
        "        1. Provide a concise and accurate answer.\n",
        "        2. Be specific. If possible, mention the policy name or number.\n",
        "        3. Do not make up any information not present in the context.\n",
        "        4. If the context is about a specific jurisdiction, note it in your answer.\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "\n",
        "    def _generate_answer(self, prompt):\n",
        "        \"\"\"Generate an answer using Vertex AI's Gemini model.\"\"\"\n",
        "        try:\n",
        "            response = self.llm_model.predict(\n",
        "                prompt,\n",
        "                temperature=0.1, # Low temperature for factual accuracy\n",
        "                max_output_tokens=1024,\n",
        "                top_k=40,\n",
        "                top_p=0.8\n",
        "            )\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating answer: {e}\")\n",
        "            return \"An error occurred while generating an answer.\"\n",
        "\n",
        "    def _log_query(self, user_id, query, chunks, answer):\n",
        "        \"\"\"Log the query and results to BigQuery for analytics.\"\"\"\n",
        "        # ... Implementation to insert into a BigQuery table\n",
        "        pass\n",
        "\n",
        "    async def _generate_embedding(self, text):\n",
        "        \"\"\"Generate an embedding for the given text using Vertex AI.\"\"\"\n",
        "        # ... Implementation using textembedding-gecko model\n",
        "        pass"
      ],
      "metadata": {
        "id": "jmVmCDDv2zNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Deployment & Orchestration\n",
        "A simple deployment script (scripts/deploy.sh) would look like this:"
      ],
      "metadata": {
        "id": "hdpmnedW249L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "\n",
        "# Deploy Infrastructure\n",
        "cd infra/\n",
        "terraform init\n",
        "terraform apply -auto-approve -var=\"project_id=$PROJECT_ID\"\n",
        "\n",
        "# Deploy Dataflow Pipeline\n",
        "cd ../ingestion/\n",
        "python -m beam_pipeline --runner DataflowRunner \\\n",
        "    --project $PROJECT_ID \\\n",
        "    --region us-central1 \\\n",
        "    --temp_location gs://$BUCKET_NAME/temp \\\n",
        "    --setup_file ./setup.py\n",
        "\n",
        "# Build and Deploy Cloud Run Service\n",
        "cd ../api/\n",
        "gcloud builds submit --tag gcr.io/$PROJECT_ID/rag-api\n",
        "gcloud run deploy rag-api \\\n",
        "    --image gcr.io/$PROJECT_ID/rag-api \\\n",
        "    --platform managed \\\n",
        "    --region us-central1 \\\n",
        "    --service-account rag-service-account@$PROJECT_ID.iam.gserviceaccount.com \\\n",
        "    --set-env-vars PROJECT_ID=$PROJECT_ID \\\n",
        "    --vpc-connector my-vpc-connector \\\n",
        "    --ingress internal-and-cloud-load-balancing"
      ],
      "metadata": {
        "id": "qT1MNQUg26p1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}